{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c57b89ea-ea57-4a0e-841a-a19fc83c487c",
   "metadata": {},
   "source": [
    "## Dolly 3B fine tuning through PEFT/LoRA - Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d14eef-4b1b-4b09-b452-a46be44535bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install datasets\n",
    "#!pip install gradio\n",
    "#!pip install py7zr\n",
    "#!pip install accelerate\n",
    "#!pip install bitsandbytes\n",
    "#!pip install peft\n",
    "#!pip install rouge_score\n",
    "#!pip install evaluate\n",
    "\n",
    "#!pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f508b790-69f0-4d79-8eb9-c17ef3899f16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e63fdd-b89f-4b23-bc1f-318ca7b6d096",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling, DataCollatorForSeq2Seq\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d56e633-bb37-4c33-898d-625ec82b2714",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re, os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ff11d2-3b0b-4af4-959d-7a05c34555a5",
   "metadata": {},
   "source": [
    "### Load LoRA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f58d29-63d2-4678-844c-d5f174e79659",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_MODEL = 'databricks/dolly-v2-3b'\n",
    "OUTPUT_DIR = 'models/dolly3b-loraft-r4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af213f0-c53d-4e5d-b4e7-f8e86023ec7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device_map = \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f976a0a-d88e-4623-a539-ccc0b8da30ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "config = PeftConfig.from_pretrained(OUTPUT_DIR)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00c8a46-0357-4732-8feb-470fb8422e75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer_ft = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer_ft.pad_token = tokenizer_ft.eos_token\n",
    "tokenizer_ft.add_special_tokens({\"additional_special_tokens\": [END_KEY, INSTRUCTION_KEY, RESPONSE_KEY_NL]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30570c7d-a8e8-48f9-8065-3297d2e6be6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_ft = AutoModelForCausalLM.from_pretrained(  \n",
    "    config.base_model_name_or_path,  \n",
    "    load_in_8bit=True,\n",
    "    device_map=device_map,\n",
    "    #cache_dir=\"/LLM_test/hf_cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecef26b-5771-40b2-9ae2-7f0204f730bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_ft = PeftModel.from_pretrained(\n",
    "    model_ft, \n",
    "    OUTPUT_DIR, \n",
    "    device_map=device_map,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609b8d92-bc64-4e73-a190-7037198aa1dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_ft.resize_token_embeddings(len(tokenizer_ft))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad1256e-0ba9-4777-bf6b-60652f280940",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc1bc74-da01-4772-bea1-b02594628c80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fe68cdd-e4c6-4f22-bd25-a2ef0d0a51d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inference using instruct pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0903dd44-7dc3-4047-b015-42513421686d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Instruct Pipeline\n",
    "import logging\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "from transformers import Pipeline, PreTrainedTokenizer\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# This is the prompt that is used for generating responses using an already trained model.  It ends with the response\n",
    "# key, where the job of the model is to provide the completion that follows it (i.e. the response itself).\n",
    "PROMPT_FOR_GENERATION_FORMAT = \"\"\"{intro}\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "{response_key}\n",
    "\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    ")\n",
    "\n",
    "def get_special_token_id(tokenizer: PreTrainedTokenizer, key: str) -> int:\n",
    "    \"\"\"Gets the token ID for a given string that has been added to the tokenizer as a special token.\n",
    "    When training, we configure the tokenizer so that the sequences like \"### Instruction:\" and \"### End\" are\n",
    "    treated specially and converted to a single, new token.  This retrieves the token ID each of these keys map to.\n",
    "    Args:\n",
    "        tokenizer (PreTrainedTokenizer): the tokenizer\n",
    "        key (str): the key to convert to a single token\n",
    "    Raises:\n",
    "        RuntimeError: if more than one ID was generated\n",
    "    Returns:\n",
    "        int: the token ID for the given key\n",
    "    \"\"\"\n",
    "    token_ids = tokenizer.encode(key)\n",
    "    if len(token_ids) > 1:\n",
    "        raise ValueError(f\"Expected only a single token for '{key}' but found {token_ids}\")\n",
    "    return token_ids[0]\n",
    "\n",
    "\n",
    "class InstructionTextGenerationPipeline(Pipeline):\n",
    "    def __init__(\n",
    "        self, *args, do_sample: bool = True, max_new_tokens: int = 256, top_p: float = 0.92, top_k: int = 0, **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, do_sample=do_sample, max_new_tokens=max_new_tokens, top_p=top_p, top_k=top_k, **kwargs)\n",
    "\n",
    "    def _sanitize_parameters(self, return_instruction_text=False, **generate_kwargs):\n",
    "        preprocess_params = {}\n",
    "\n",
    "        # newer versions of the tokenizer configure the response key as a special token.  newer versions still may\n",
    "        # append a newline to yield a single token.  find whatever token is configured for the response key.\n",
    "        tokenizer_response_key = next(\n",
    "            (token for token in self.tokenizer.additional_special_tokens if token.startswith(RESPONSE_KEY)), None\n",
    "        )\n",
    "\n",
    "        response_key_token_id = None\n",
    "        end_key_token_id = None\n",
    "        if tokenizer_response_key:\n",
    "            try:\n",
    "                response_key_token_id = get_special_token_id(self.tokenizer, tokenizer_response_key)\n",
    "                end_key_token_id = get_special_token_id(self.tokenizer, END_KEY)\n",
    "\n",
    "                # Ensure generation stops once it generates \"### End\"\n",
    "                generate_kwargs[\"eos_token_id\"] = end_key_token_id\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        forward_params = generate_kwargs\n",
    "        postprocess_params = {\n",
    "            \"response_key_token_id\": response_key_token_id,\n",
    "            \"end_key_token_id\": end_key_token_id,\n",
    "            \"return_instruction_text\": return_instruction_text,\n",
    "        }\n",
    "\n",
    "        return preprocess_params, forward_params, postprocess_params\n",
    "\n",
    "    def preprocess(self, instruction_text, **generate_kwargs):\n",
    "        prompt_text = PROMPT_FOR_GENERATION_FORMAT.format(instruction=instruction_text)\n",
    "        inputs = self.tokenizer(\n",
    "            prompt_text,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs[\"prompt_text\"] = prompt_text\n",
    "        inputs[\"instruction_text\"] = instruction_text\n",
    "        return inputs\n",
    "\n",
    "    def _forward(self, model_inputs, **generate_kwargs):\n",
    "        input_ids = model_inputs[\"input_ids\"]\n",
    "        attention_mask = model_inputs.get(\"attention_mask\", None)\n",
    "        generated_sequence = self.model.generate(\n",
    "            input_ids=input_ids.to(self.model.device),\n",
    "            attention_mask=attention_mask,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            **generate_kwargs,\n",
    "        )[0].cpu()\n",
    "        instruction_text = model_inputs.pop(\"instruction_text\")\n",
    "        return {\"generated_sequence\": generated_sequence, \"input_ids\": input_ids, \"instruction_text\": instruction_text}\n",
    "\n",
    "    def postprocess(self, model_outputs, response_key_token_id, end_key_token_id, return_instruction_text):\n",
    "        sequence = model_outputs[\"generated_sequence\"]\n",
    "        instruction_text = model_outputs[\"instruction_text\"]\n",
    "\n",
    "        # The response will be set to this variable if we can identify it.\n",
    "        decoded = None\n",
    "\n",
    "        # If we have token IDs for the response and end, then we can find the tokens and only decode between them.\n",
    "        if response_key_token_id and end_key_token_id:\n",
    "            # Find where \"### Response:\" is first found in the generated tokens.  Considering this is part of the\n",
    "            # prompt, we should definitely find it.  We will return the tokens found after this token.\n",
    "            response_pos = None\n",
    "            response_positions = np.where(sequence == response_key_token_id)[0]\n",
    "            if len(response_positions) == 0:\n",
    "                logger.warn(f\"Could not find response key {response_key_token_id} in: {sequence}\")\n",
    "            else:\n",
    "                response_pos = response_positions[0]\n",
    "\n",
    "            if response_pos:\n",
    "                # Next find where \"### End\" is located.  The model has been trained to end its responses with this\n",
    "                # sequence (or actually, the token ID it maps to, since it is a special token).  We may not find\n",
    "                # this token, as the response could be truncated.  If we don't find it then just return everything\n",
    "                # to the end.  Note that even though we set eos_token_id, we still see the this token at the end.\n",
    "                end_pos = None\n",
    "                end_positions = np.where(sequence == end_key_token_id)[0]\n",
    "                if len(end_positions) > 0:\n",
    "                    end_pos = end_positions[0]\n",
    "\n",
    "                decoded = self.tokenizer.decode(sequence[response_pos + 1 : end_pos]).strip()\n",
    "        else:\n",
    "            # Otherwise we'll decode everything and use a regex to find the response and end.\n",
    "\n",
    "            fully_decoded = self.tokenizer.decode(sequence)\n",
    "\n",
    "            # The response appears after \"### Response:\".  The model has been trained to append \"### End\" at the\n",
    "            # end.\n",
    "            m = re.search(r\"#+\\s*Response:\\s*(.+?)#+\\s*End\", fully_decoded, flags=re.DOTALL)\n",
    "\n",
    "            if m:\n",
    "                decoded = m.group(1).strip()\n",
    "            else:\n",
    "                # The model might not generate the \"### End\" sequence before reaching the max tokens.  In this case,\n",
    "                # return everything after \"### Response:\".\n",
    "                m = re.search(r\"#+\\s*Response:\\s*(.+)\", fully_decoded, flags=re.DOTALL)\n",
    "                if m:\n",
    "                    decoded = m.group(1).strip()\n",
    "                else:\n",
    "                    logger.warn(f\"Failed to find response in:\\n{fully_decoded}\")\n",
    "\n",
    "        if return_instruction_text:\n",
    "            return {\"instruction_text\": instruction_text, \"generated_text\": decoded}\n",
    "\n",
    "        return decoded\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6ee2c6-c884-4490-9583-ff0016497a7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dolly_gen = InstructionTextGenerationPipeline(model=model_ft, tokenizer=tokenizer_ft)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c776f586-83a3-4b04-90a0-947b911ec282",
   "metadata": {},
   "source": [
    "Single query test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c818278-a1ac-4b83-9cc8-c081ebbf6054",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruction = \"Do I need to look out for an email with instructions?\"\n",
    "\n",
    "response = dolly_gen(instruction)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833a0e14-af2f-4a2f-85de-a75dd19b17ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc870811-1e3e-4592-abb1-3c32e4a5bdc9",
   "metadata": {},
   "source": [
    "### Batch inference test and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbce9820-19fa-4281-b88b-845aedb6c215",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea54792-33bf-4ed4-adf7-24d64de112cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_FILE = 'mlu_ops_concat_all_df_list_instruction_output_1441.json'\n",
    "OUTPUT_FILE = 'responses_dolly3bloraft_r4inf.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcaf7d1-1864-435d-9f82-8eab08fe2bea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_results = pd.read_json(INPUT_FILE)\n",
    "df_results = df_results.sample(n=200, random_state=123)\n",
    "prompt_list = df_results.instruction.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4953bcc-bd14-4336-8e68-8af3429b92b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_list = []\n",
    "\n",
    "newline, bold, unbold = \"\\n\", \"\\033[1m\", \"\\033[0m\"\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "i=0\n",
    "for text in prompt_list:\n",
    "    print(i,\" \",end = '')\n",
    "    response = dolly_gen(text)\n",
    "    response_list.append(response)\n",
    "    i = i+1\n",
    "\n",
    "et = time.time()\n",
    "elapsed_time = et - st\n",
    "print('Execution time:', elapsed_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75823ad4-9dbe-4f7f-807c-fe36134db420",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_response = pd.DataFrame()  \n",
    "df_response[\"prompt\"] = prompt_list\n",
    "df_response[\"dolly3bloraft-r40\"] = response_list\n",
    "df_response.to_csv(OUTPUT_FILE, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a4669c-e7bd-4179-b3bf-262a1b651517",
   "metadata": {},
   "source": [
    "Performance evaluation via ROUGE SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4995e4e9-44fd-4197-89ef-2da72d269937",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import evaluate\n",
    "rouge = evaluate.load('rouge')\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2589a011-ef5b-462f-9111-5db6a67cd41b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_rouge(pred_list,ref_list):\n",
    "    \n",
    "    rouge_results = rouge.compute(predictions=pred_list,\n",
    "                         references=ref_list,\n",
    "                         use_aggregator=True)\n",
    "    avg_rougeLsum = np.mean(rouge_results[\"rougeLsum\"])\n",
    "    avg_rougeL = np.mean(rouge_results[\"rougeL\"])\n",
    "    avg_rouge2 = np.mean(rouge_results[\"rouge2\"])\n",
    "    avg_rouge1 = np.mean(rouge_results[\"rouge1\"])\n",
    "    \n",
    "    print(\"Average rouge scores: \", avg_rougeLsum, avg_rougeL, avg_rouge2, avg_rouge1)\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607214e0-67dc-485a-98e1-bccfbe623fef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ref_list = list(df_results[\"output\"])\n",
    "dolly3b_list = list(df_response[\"dolly3bloraft-r10\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5caac65-a128-440c-a58c-6013e811c9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_rouge(dolly3b_list, ref_list)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a16091-f869-4115-9fdb-c6929246d328",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/pytorch-2.0.0-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
