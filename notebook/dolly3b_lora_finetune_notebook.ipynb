{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c57b89ea-ea57-4a0e-841a-a19fc83c487c",
   "metadata": {},
   "source": [
    "## Dolly 3B fine tuning through PEFT/LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d14eef-4b1b-4b09-b452-a46be44535bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install datasets\n",
    "#!pip install gradio\n",
    "#!pip install py7zr\n",
    "#!pip install accelerate\n",
    "#!pip install bitsandbytes\n",
    "#!pip install peft\n",
    "#!pip install rouge_score\n",
    "#!pip install evaluate\n",
    "\n",
    "#!pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f508b790-69f0-4d79-8eb9-c17ef3899f16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e63fdd-b89f-4b23-bc1f-318ca7b6d096",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling, DataCollatorForSeq2Seq\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d56e633-bb37-4c33-898d-625ec82b2714",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re, os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b9de76-8226-47e3-beca-6c682b3a3e43",
   "metadata": {},
   "source": [
    "### Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8878cb4a-2b09-4bb0-b5bd-1ce6ab3c8e02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e926447f-997b-4cf9-a7fe-6265be2dee5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAINING_FILE = 'data/mlu_ops_concat_all_df_list_instruction_output_1441.json'\n",
    "dataset = load_dataset(\"json\", data_files=TRAINING_FILE)[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4081543-2487-47f2-9d8f-bcef1b39cdac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.rename_column(\"output\", \"response\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d69609-cb2c-4f32-a830-8a8d3a658e2e",
   "metadata": {},
   "source": [
    "Deine special token for instruction tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4f7be7-8be0-43e9-aa84-d0c680461942",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INTRO_BLURB = (\n",
    "    \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    ")\n",
    "INSTRUCTION_KEY = \"### Instruction:\"\n",
    "INPUT_KEY = \"Input:\"\n",
    "RESPONSE_KEY = \"### Response:\"\n",
    "END_KEY = \"### End\"\n",
    "RESPONSE_KEY_NL = f\"{RESPONSE_KEY}\\n\"\n",
    "DEFAULT_SEED = 42\n",
    "\n",
    "# This is a training prompt that does not contain an input string.  The instruction by itself has enough information\n",
    "# to respond.  For example, the instruction might ask for the year a historic figure was born.\n",
    "PROMPT_NO_INPUT_FORMAT = \"\"\"{intro}\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "{response_key}\n",
    "{response}\n",
    "{end_key}\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    "    response=\"{response}\",\n",
    "    end_key=END_KEY,\n",
    ")\n",
    "\n",
    "# This is a training prompt that contains an input string that serves as context for the instruction.  For example,\n",
    "# the input might be a passage from Wikipedia and the intruction is to extract some information from it.\n",
    "PROMPT_WITH_INPUT_FORMAT = \"\"\"{intro}\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "{input_key}\n",
    "{input}\n",
    "{response_key}\n",
    "{response}\n",
    "{end_key}\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    input_key=INPUT_KEY,\n",
    "    input=\"{input}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    "    response=\"{response}\",\n",
    "    end_key=END_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc18dae9-cb02-481f-a232-214de5ba93a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "Define DataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9cdf7d-8e72-47bc-a8ee-2bf0908a58ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1dff40-d856-4106-8470-4364e0df5fef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataCollatorForCompletionOnlyLM(DataCollatorForLanguageModeling):\n",
    "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        batch = super().torch_call(examples)\n",
    "\n",
    "        # The prompt ends with the response key plus a newline.  We encode this and then try to find it in the\n",
    "        # sequence of tokens.  This should just be a single token.\n",
    "        response_token_ids = self.tokenizer.encode(RESPONSE_KEY_NL)\n",
    "\n",
    "        labels = batch[\"labels\"].clone()\n",
    "\n",
    "        for i in range(len(examples)):\n",
    "\n",
    "            response_token_ids_start_idx = None\n",
    "            for idx in np.where(batch[\"labels\"][i] == response_token_ids[0])[0]:\n",
    "                response_token_ids_start_idx = idx\n",
    "                break\n",
    "\n",
    "            if response_token_ids_start_idx is None:\n",
    "                raise RuntimeError(\n",
    "                    f'Could not find response key {response_token_ids} in token IDs {batch[\"labels\"][i]}'\n",
    "                )\n",
    "\n",
    "            response_token_ids_end_idx = response_token_ids_start_idx + 1\n",
    "\n",
    "            # Make pytorch loss function ignore all tokens up through the end of the response key\n",
    "            labels[i, :response_token_ids_end_idx] = -100\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffcf5f0-19f9-42eb-8d45-e2f8067734b3",
   "metadata": {},
   "source": [
    "Add processed data attribute in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fa0a3c-1700-4577-b002-a7f53d02e3b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _add_text(rec):\n",
    "        instruction = rec[\"instruction\"]\n",
    "        response = rec[\"response\"]\n",
    "        context = rec.get(\"context\")\n",
    "        \n",
    "        if not instruction:\n",
    "            raise ValueError(f\"Expected an instruction in: {rec}\")\n",
    "\n",
    "        if not response:\n",
    "            raise ValueError(f\"Expected a response in: {rec}\")\n",
    "\n",
    "        # For some instructions there is an input that goes along with the instruction, providing context for the\n",
    "        # instruction.  For example, the input might be a passage from Wikipedia and the instruction says to extract\n",
    "        # some piece of information from it.  The response is that information to extract.  In other cases there is\n",
    "        # no input.  For example, the instruction might be open QA such as asking what year some historic figure was\n",
    "        # born.\n",
    "        if context:\n",
    "            rec[\"text\"] = PROMPT_WITH_INPUT_FORMAT.format(instruction=instruction, response=response, input=context)\n",
    "        else:\n",
    "            rec[\"text\"] = PROMPT_NO_INPUT_FORMAT.format(instruction=instruction, response=response)\n",
    "        \n",
    "        return rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8d1227-5bd0-4a73-9135-d89f32a78c11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(_add_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f335b4-5ac9-4fc1-867e-0b877998406a",
   "metadata": {},
   "source": [
    "### Load model in 8bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d0012a-6a10-439a-a9e1-544013b87b9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8196909-7d27-4924-b2f2-1d72da0a9a1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_MODEL = 'databricks/dolly-v2-3b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dacfcf0-95a4-4190-8c74-e39aef5df861",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device_map=\"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149623e3-9db7-4e2b-a029-b254a0bb7ddd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    BASE_MODEL, \n",
    "    #cache_dir=\"/LLM_test/hf_cache\"\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [END_KEY, INSTRUCTION_KEY, RESPONSE_KEY_NL]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e219f53-5d03-4069-af4e-6e559b8ee0c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f205465d-ec63-46d1-ab65-e02d5c652325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(       # other models\n",
    "    BASE_MODEL, \n",
    "    use_cache=False,\n",
    "    device_map=device_map,\n",
    "    load_in_8bit=True,\n",
    "    #cache_dir=\"/LLM_test/hf_cache\"\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa733da8-713e-4e3f-9e08-ae618289e8f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6880850c-1568-40a1-ab0f-7abf3cd8d70e",
   "metadata": {},
   "source": [
    "### Create instruct Pipeline for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bf10ea-be45-4b3b-ad52-61457ff30d35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Instruct Pipeline\n",
    "import logging\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "from transformers import Pipeline, PreTrainedTokenizer\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# This is the prompt that is used for generating responses using an already trained model.  It ends with the response\n",
    "# key, where the job of the model is to provide the completion that follows it (i.e. the response itself).\n",
    "PROMPT_FOR_GENERATION_FORMAT = \"\"\"{intro}\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "{response_key}\n",
    "\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    ")\n",
    "\n",
    "def get_special_token_id(tokenizer: PreTrainedTokenizer, key: str) -> int:\n",
    "    \"\"\"Gets the token ID for a given string that has been added to the tokenizer as a special token.\n",
    "    When training, we configure the tokenizer so that the sequences like \"### Instruction:\" and \"### End\" are\n",
    "    treated specially and converted to a single, new token.  This retrieves the token ID each of these keys map to.\n",
    "    Args:\n",
    "        tokenizer (PreTrainedTokenizer): the tokenizer\n",
    "        key (str): the key to convert to a single token\n",
    "    Raises:\n",
    "        RuntimeError: if more than one ID was generated\n",
    "    Returns:\n",
    "        int: the token ID for the given key\n",
    "    \"\"\"\n",
    "    token_ids = tokenizer.encode(key)\n",
    "    if len(token_ids) > 1:\n",
    "        raise ValueError(f\"Expected only a single token for '{key}' but found {token_ids}\")\n",
    "    return token_ids[0]\n",
    "\n",
    "\n",
    "class InstructionTextGenerationPipeline(Pipeline):\n",
    "    def __init__(\n",
    "        self, *args, do_sample: bool = True, max_new_tokens: int = 256, top_p: float = 0.92, top_k: int = 0, **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, do_sample=do_sample, max_new_tokens=max_new_tokens, top_p=top_p, top_k=top_k, **kwargs)\n",
    "\n",
    "    def _sanitize_parameters(self, return_instruction_text=False, **generate_kwargs):\n",
    "        preprocess_params = {}\n",
    "\n",
    "        # newer versions of the tokenizer configure the response key as a special token.  newer versions still may\n",
    "        # append a newline to yield a single token.  find whatever token is configured for the response key.\n",
    "        tokenizer_response_key = next(\n",
    "            (token for token in self.tokenizer.additional_special_tokens if token.startswith(RESPONSE_KEY)), None\n",
    "        )\n",
    "\n",
    "        response_key_token_id = None\n",
    "        end_key_token_id = None\n",
    "        if tokenizer_response_key:\n",
    "            try:\n",
    "                response_key_token_id = get_special_token_id(self.tokenizer, tokenizer_response_key)\n",
    "                end_key_token_id = get_special_token_id(self.tokenizer, END_KEY)\n",
    "\n",
    "                # Ensure generation stops once it generates \"### End\"\n",
    "                generate_kwargs[\"eos_token_id\"] = end_key_token_id\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        forward_params = generate_kwargs\n",
    "        postprocess_params = {\n",
    "            \"response_key_token_id\": response_key_token_id,\n",
    "            \"end_key_token_id\": end_key_token_id,\n",
    "            \"return_instruction_text\": return_instruction_text,\n",
    "        }\n",
    "\n",
    "        return preprocess_params, forward_params, postprocess_params\n",
    "\n",
    "    def preprocess(self, instruction_text, **generate_kwargs):\n",
    "        prompt_text = PROMPT_FOR_GENERATION_FORMAT.format(instruction=instruction_text)\n",
    "        inputs = self.tokenizer(\n",
    "            prompt_text,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs[\"prompt_text\"] = prompt_text\n",
    "        inputs[\"instruction_text\"] = instruction_text\n",
    "        return inputs\n",
    "\n",
    "    def _forward(self, model_inputs, **generate_kwargs):\n",
    "        input_ids = model_inputs[\"input_ids\"]\n",
    "        attention_mask = model_inputs.get(\"attention_mask\", None)\n",
    "        generated_sequence = self.model.generate(\n",
    "            input_ids=input_ids.to(self.model.device),\n",
    "            attention_mask=attention_mask,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            **generate_kwargs,\n",
    "        )[0].cpu()\n",
    "        instruction_text = model_inputs.pop(\"instruction_text\")\n",
    "        return {\"generated_sequence\": generated_sequence, \"input_ids\": input_ids, \"instruction_text\": instruction_text}\n",
    "\n",
    "    def postprocess(self, model_outputs, response_key_token_id, end_key_token_id, return_instruction_text):\n",
    "        sequence = model_outputs[\"generated_sequence\"]\n",
    "        instruction_text = model_outputs[\"instruction_text\"]\n",
    "\n",
    "        # The response will be set to this variable if we can identify it.\n",
    "        decoded = None\n",
    "\n",
    "        # If we have token IDs for the response and end, then we can find the tokens and only decode between them.\n",
    "        if response_key_token_id and end_key_token_id:\n",
    "            # Find where \"### Response:\" is first found in the generated tokens.  Considering this is part of the\n",
    "            # prompt, we should definitely find it.  We will return the tokens found after this token.\n",
    "            response_pos = None\n",
    "            response_positions = np.where(sequence == response_key_token_id)[0]\n",
    "            if len(response_positions) == 0:\n",
    "                logger.warn(f\"Could not find response key {response_key_token_id} in: {sequence}\")\n",
    "            else:\n",
    "                response_pos = response_positions[0]\n",
    "\n",
    "            if response_pos:\n",
    "                # Next find where \"### End\" is located.  The model has been trained to end its responses with this\n",
    "                # sequence (or actually, the token ID it maps to, since it is a special token).  We may not find\n",
    "                # this token, as the response could be truncated.  If we don't find it then just return everything\n",
    "                # to the end.  Note that even though we set eos_token_id, we still see the this token at the end.\n",
    "                end_pos = None\n",
    "                end_positions = np.where(sequence == end_key_token_id)[0]\n",
    "                if len(end_positions) > 0:\n",
    "                    end_pos = end_positions[0]\n",
    "\n",
    "                decoded = self.tokenizer.decode(sequence[response_pos + 1 : end_pos]).strip()\n",
    "        else:\n",
    "            # Otherwise we'll decode everything and use a regex to find the response and end.\n",
    "\n",
    "            fully_decoded = self.tokenizer.decode(sequence)\n",
    "\n",
    "            # The response appears after \"### Response:\".  The model has been trained to append \"### End\" at the\n",
    "            # end.\n",
    "            m = re.search(r\"#+\\s*Response:\\s*(.+?)#+\\s*End\", fully_decoded, flags=re.DOTALL)\n",
    "\n",
    "            if m:\n",
    "                decoded = m.group(1).strip()\n",
    "            else:\n",
    "                # The model might not generate the \"### End\" sequence before reaching the max tokens.  In this case,\n",
    "                # return everything after \"### Response:\".\n",
    "                m = re.search(r\"#+\\s*Response:\\s*(.+)\", fully_decoded, flags=re.DOTALL)\n",
    "                if m:\n",
    "                    decoded = m.group(1).strip()\n",
    "                else:\n",
    "                    logger.warn(f\"Failed to find response in:\\n{fully_decoded}\")\n",
    "\n",
    "        if return_instruction_text:\n",
    "            return {\"instruction_text\": instruction_text, \"generated_text\": decoded}\n",
    "\n",
    "        return decoded\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d38a23-ff4a-4ba8-9881-b025b24960f3",
   "metadata": {},
   "source": [
    "Single query test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f92a8c9-c4d0-43d3-88f4-4062fe27ff03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruction = \"What are the requirements before registering MLU courses?\"\n",
    "dolly_gen = InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer)\n",
    "response = dolly_gen(instruction)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb08c7bb-73a7-4b33-a7fc-ac4644ab1b84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10ba45d8-acfe-40ec-8154-d544c627be14",
   "metadata": {},
   "source": [
    "### Fine Tuning through LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfdcd3c-492b-43bb-b5d0-13537934de2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_batch(batch: Dict[str, List], tokenizer: AutoTokenizer, max_length: int) -> dict:\n",
    "    model_inputs = tokenizer(batch[\"text\"],max_length=max_length,truncation=True,)\n",
    "\n",
    "    return model_inputs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b54d8e-e31c-4f68-8cee-d44c333f827e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "_preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2b22bb-ff62-4bde-83db-e93a5c2e5b0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"instruction\", \"input\",\"response\", \"text\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e4d174-c8e9-4568-86d6-0ef78f4d1158",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed_dataset = dataset.filter(lambda rec: len(rec[\"input_ids\"]) < max_length)\n",
    "#processed_dataset = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbed33be-e25e-4f49-b09c-087b0bfebc13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_dataset = processed_dataset.train_test_split(test_size=41, seed=0)\n",
    "split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ed8750-2887-4420-a5d4-77aab06796a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a430e272-dcc2-4d1f-b906-dd2d53418045",
   "metadata": {},
   "source": [
    "Prepare int-8 model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c4f0bb-176b-49c0-b567-c070450604db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = prepare_model_for_int8_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c624c1d8-d0d6-4588-b648-18a3b6a0d7b4",
   "metadata": {},
   "source": [
    "Define LoRA Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29684ba-8a77-4f7e-9ccd-4112363968dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    " r=512,             #  256,     #64,     #16\n",
    " lora_alpha=1024,       # 512,   #32,      \n",
    " lora_dropout=0.05,      #0.05,\n",
    " bias=\"none\",\n",
    " task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b2d4ec-e135-40ec-bf97-46c39acdc0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add LoRA adaptor\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8645ce7-3844-4cf7-bae6-98caf716a800",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForCompletionOnlyLM(\n",
    "        tokenizer=tokenizer, mlm=False, return_tensors=\"pt\", pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8de81c1-9e22-4bf3-a9ba-f7aff92a1256",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIR = 'dolly3b-lora-ft-r10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be77036-38d4-4f5b-a4c4-3215640a996e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(    \n",
    "        output_dir=OUTPUT_DIR,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        #predict_with_generate=True,\n",
    "        #weight_decay=0.01,\n",
    "        #fp16=False,                    \n",
    "        #bf16=True,\n",
    "        learning_rate=1e-4,\n",
    "        num_train_epochs=1,      # 10,\n",
    "        #deepspeed=None,\n",
    "        #gradient_checkpointing=False,\n",
    "        #logging_dir=f\"{local_output_dir}/runs\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=100,    # 50\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=100,       # 50\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=20000,\n",
    "        save_total_limit=10,\n",
    "        #load_best_model_at_end=True,\n",
    "        #report_to=\"tensorboard\",\n",
    "        #disable_tqdm=True,\n",
    "        #remove_unused_columns=False,\n",
    "        #local_rank=True,\n",
    "        #warmup_steps=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102f71e3-632d-49ea-9609-163dca9373ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(    \n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        train_dataset=split_dataset[\"train\"],\n",
    "        eval_dataset=split_dataset[\"test\"],\n",
    "        data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f765fc-2e67-4810-99bf-d1a0750fa338",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33af6aea-a21e-4170-a71d-c9725e6853c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if BASE_MODEL == \"databricks/dolly-v2-3b\":\n",
    "    model = model.half()\n",
    "    model = model.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2fc1b1-e11b-41a2-b2c9-170a787ef894",
   "metadata": {},
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aa9aae-9d71-4534-ba62-6a089fb81ff4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "et = time.time()\n",
    "elapsed_time = et - st\n",
    "print('Training time:', elapsed_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6f1fd5-b2a1-462b-bd38-ce267a7c0aeb",
   "metadata": {},
   "source": [
    "Inference the trained model with single query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261a2d7d-d560-450c-aad0-b3fbf4c83a22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruction = \"What is the minimum I need to do to pass?\"\n",
    "dolly_gen = InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer)\n",
    "response = dolly_gen(instruction)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441543ba-1e30-4362-b024-83897b3a442b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a16091-f869-4115-9fdb-c6929246d328",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/pytorch-2.0.0-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
